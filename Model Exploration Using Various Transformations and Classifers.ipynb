{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "import numpy as np\n",
    "import featuretools as ft\n",
    "import featuretools.variable_types as vtypes\n",
    "from featuretools.primitives import *\n",
    "from nlp_primitives import (\n",
    "    DiversityScore,\n",
    "    LSA,\n",
    "    MeanCharactersPerWord,\n",
    "    PartOfSpeechCount,\n",
    "    PolarityScore, \n",
    "    PunctuationCount,\n",
    "    StopwordCount,\n",
    "    TitleWordCount,\n",
    "    UniversalSentenceEncoder,\n",
    "    UpperCaseCount)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tweetsAndLabels = pd.read_csv('MajorityVote.csv')\n",
    "tweetsAndLabels['Created'] = pd.to_datetime(tweetsAndLabels['Created'])\n",
    "#tweetsAndLabels.set_index('Created',inplace = True)\n",
    "tweetsAndLabels = tweetsAndLabels.iloc[0:3499]\n",
    "#replacing NAs with '' for concatenating 'Full Text' and 'Quoted Full Text'\n",
    "tweetsAndLabels = tweetsAndLabels.replace(np.nan, '', regex=True)\n",
    "#concatenating 'Full Text' and 'Quoted Full Text'\n",
    "tweetsAndLabels['Full Text'] = tweetsAndLabels['Full Text'] + ' ' + tweetsAndLabels['Quote Full Text']\n",
    "\n",
    "#Getting the four HBM Label Categories\n",
    "categories = list(tweetsAndLabels.columns.values)\n",
    "categories = categories[7:12]\n",
    "\n",
    "\n",
    "#def clean_text(text):\n",
    "  #  text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "   # tokens = re.split('\\W+', text)\n",
    "  #  text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "  #  return text\n",
    "\n",
    "#cl#ean_text(tweetsA)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining clean_text function\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def clean_textTwo(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = \"\".join([word.lower() for word in text if word not in punctuation])\n",
    "    tokens = tknzr.tokenize(text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating functions that generates new features\n",
    "DS = DiversityScore()\n",
    "MCPW = MeanCharactersPerWord()\n",
    "Pol = PolarityScore()\n",
    "PC = PunctuationCount()\n",
    "SC = StopwordCount()\n",
    "TWC = TitleWordCount()\n",
    "UCC = UpperCaseCount()\n",
    "USE = UniversalSentenceEncoder()\n",
    "\n",
    "#defining the function that checks for free standing numbers\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\s\\d+\\s', inputString))\n",
    "\n",
    "\n",
    "#adding new features to the dataframe\n",
    "tweetsAndLabels['hasNum'] = tweetsAndLabels['Full Text'].apply(lambda x:hasNumbers(x))\n",
    "NewFeaturesFunct = [DS,MCPW,Pol,PC,SC,TWC,UCC]\n",
    "featureNames = ['DS','MCPW','Pol','PC','SC','TWC','UCC']\n",
    "for featureName, featureFunct in zip(featureNames,NewFeaturesFunct):\n",
    "    tweetsAndLabels[featureName] = featureFunct(tweetsAndLabels['Full Text'])\n",
    "tweetsAndLabels = tweetsAndLabels.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2448, 21)\n",
      "(1050, 21)\n",
      "(2448, 21)\n",
      "(1050, 21)\n"
     ]
    }
   ],
   "source": [
    "#Train Test Split\n",
    "HBMRelatedTweets = tweetsAndLabels.loc[tweetsAndLabels['HBM Related']==1.0]\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(tweetsAndLabels, random_state=42, test_size=0.3, shuffle=True)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "train_text = train['Full Text']\n",
    "test_text = test['Full Text']\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 8)\n",
      "(1050, 5711)\n",
      "(2448, 8)\n",
      "(2448, 5711)\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(analyzer=clean_textTwo)\n",
    "#vectorizer = CountVectorizer(analyzer=clean_textTwo)\n",
    "vectorizer.fit(train_text)\n",
    "vectorizer.fit(test_text)\n",
    "newFeatures_train = train[['DS','MCPW','Pol','PC','SC','TWC','UCC','hasNum']]\n",
    "newFeatures_test = test[['DS','MCPW','Pol','PC','SC','TWC','UCC','hasNum']]\n",
    "\n",
    "#converting sparse matrix to dataframe and preping the vectorized matrix to be concatenated with the new features\n",
    "X_tfidf_train = vectorizer.transform(train_text)\n",
    "X_tfidf_train = pd.DataFrame(X_tfidf_train.toarray())\n",
    "X_tfidf_test = vectorizer.transform(test_text)\n",
    "X_tfidf_test = pd.DataFrame(X_tfidf_test.toarray())\n",
    "X_tfidf_test.reset_index(drop=True, inplace=True)\n",
    "X_tfidf_train.reset_index(drop=True, inplace=True)\n",
    "newFeatures_test.reset_index(drop=True, inplace=True)\n",
    "newFeatures_train.reset_index(drop=True, inplace=True)\n",
    "print(newFeatures_test.shape)\n",
    "print(X_tfidf_test.shape)\n",
    "print(newFeatures_train.shape)\n",
    "print(X_tfidf_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining new features to the vectorized data frame\n",
    "#x_train = vectorizer.transform(train_text)\n",
    "x_train = pd.concat([newFeatures_train,X_tfidf_train],axis = 1)\n",
    "y_train = train.drop(labels = ['Created', 'Day', 'Tweet ID', 'Full Text', 'Quote Full Text',\n",
    "       'Hashtags', 'Not mask or face covering related','HBM Related AutoFill','HBM Related','DS','MCPW','Pol','PC','SC','TWC','UCC','hasNum'], axis=1)\n",
    "\n",
    "#x_test = vectorizer.transform(test_text)\n",
    "x_test = pd.concat([newFeatures_test,X_tfidf_test],axis = 1)\n",
    "y_test = test.drop(labels = ['Created', 'Day', 'Tweet ID', 'Full Text', 'Quote Full Text',\n",
    "       'Hashtags', 'Not mask or face covering related','HBM Related AutoFill','HBM Related','DS','MCPW','Pol','PC','SC','TWC','UCC','hasNum'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Processing HBM Related Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.44      0.54       434\n",
      "           1       0.69      0.88      0.77       616\n",
      "\n",
      "    accuracy                           0.70      1050\n",
      "   macro avg       0.70      0.66      0.66      1050\n",
      "weighted avg       0.70      0.70      0.68      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Benefits Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.96      0.85       741\n",
      "           1       0.73      0.27      0.39       309\n",
      "\n",
      "    accuracy                           0.76      1050\n",
      "   macro avg       0.74      0.61      0.62      1050\n",
      "weighted avg       0.75      0.76      0.71      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Barriers Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.98      0.85       771\n",
      "           1       0.60      0.10      0.17       279\n",
      "\n",
      "    accuracy                           0.74      1050\n",
      "   macro avg       0.67      0.54      0.51      1050\n",
      "weighted avg       0.71      0.74      0.67      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Severity Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       999\n",
      "           1       0.29      0.14      0.19        51\n",
      "\n",
      "    accuracy                           0.94      1050\n",
      "   macro avg       0.62      0.56      0.58      1050\n",
      "weighted avg       0.92      0.94      0.93      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Susceptibility Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98      1007\n",
      "           1       0.20      0.05      0.08        43\n",
      "\n",
      "    accuracy                           0.95      1050\n",
      "   macro avg       0.58      0.52      0.53      1050\n",
      "weighted avg       0.93      0.95      0.94      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ONE VS REST Classifier aka. restricting each tweet to have only one label\n",
    "#Using Gradient Boost \n",
    "#Baseline model with no resampling of minority class\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from IPython.display import Markdown, display \n",
    "\n",
    "\n",
    "# Using pipeline for applying Gradient Boosting and one vs rest classifier\n",
    "GB_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(GradientBoostingClassifier())),\n",
    "            ])\n",
    "\n",
    "for category in categories:\n",
    "    printmd('**Processing {} Tweets...**'.format(category))\n",
    "    \n",
    "    # Training GB model on train data\n",
    "    GB_pipeline.fit(x_train, train[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    predGB = GB_pipeline.predict(x_test)\n",
    "    print(classification_report(test[category], predGB, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Processing HBM Related Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.45      0.54       434\n",
      "           1       0.69      0.85      0.76       616\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.68      0.65      0.65      1050\n",
      "weighted avg       0.68      0.68      0.67      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Benefits Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.96      0.84       741\n",
      "           1       0.70      0.20      0.31       309\n",
      "\n",
      "    accuracy                           0.74      1050\n",
      "   macro avg       0.72      0.58      0.58      1050\n",
      "weighted avg       0.73      0.74      0.68      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Barriers Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.98      0.85       771\n",
      "           1       0.62      0.11      0.18       279\n",
      "\n",
      "    accuracy                           0.75      1050\n",
      "   macro avg       0.69      0.54      0.52      1050\n",
      "weighted avg       0.72      0.75      0.67      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Severity Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98       999\n",
      "           1       0.00      0.00      0.00        51\n",
      "\n",
      "    accuracy                           0.95      1050\n",
      "   macro avg       0.48      0.50      0.49      1050\n",
      "weighted avg       0.91      0.95      0.93      1050\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kylek\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kylek\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kylek\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Susceptibility Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1007\n",
      "           1       1.00      0.02      0.05        43\n",
      "\n",
      "    accuracy                           0.96      1050\n",
      "   macro avg       0.98      0.51      0.51      1050\n",
      "weighted avg       0.96      0.96      0.94      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ONE VS REST Classifier aka. restricting each tweet to have only one label\n",
    "#Using logistic regression\n",
    "#Baseline model with no resampling of minority class\n",
    "log_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(max_iter=1000))),\n",
    "            ])\n",
    "\n",
    "for category in categories:\n",
    "    printmd('**Processing {} Tweets...**'.format(category))\n",
    "    # Training logistic regression model on train data\n",
    "    log_pipeline.fit(x_train, train[category])\n",
    "    # calculating test accuracy\n",
    "    predLog = log_pipeline.predict(x_test)\n",
    "    print(classification_report(test[category], predLog, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Processing HBM Related Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.42      0.53       434\n",
      "           1       0.69      0.89      0.78       616\n",
      "\n",
      "    accuracy                           0.70      1050\n",
      "   macro avg       0.71      0.66      0.66      1050\n",
      "weighted avg       0.71      0.70      0.68      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Benefits Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.79       741\n",
      "           1       0.51      0.58      0.54       309\n",
      "\n",
      "    accuracy                           0.71      1050\n",
      "   macro avg       0.66      0.67      0.67      1050\n",
      "weighted avg       0.72      0.71      0.72      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Barriers Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.78       771\n",
      "           1       0.43      0.50      0.46       279\n",
      "\n",
      "    accuracy                           0.69      1050\n",
      "   macro avg       0.62      0.63      0.62      1050\n",
      "weighted avg       0.71      0.69      0.70      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Severity Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       999\n",
      "           1       0.37      0.45      0.40        51\n",
      "\n",
      "    accuracy                           0.94      1050\n",
      "   macro avg       0.67      0.71      0.68      1050\n",
      "weighted avg       0.94      0.94      0.94      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Susceptibility Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      1007\n",
      "           1       0.24      0.40      0.30        43\n",
      "\n",
      "    accuracy                           0.92      1050\n",
      "   macro avg       0.60      0.67      0.63      1050\n",
      "weighted avg       0.94      0.92      0.93      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#oversampling of minority class and using gradient boosting \n",
    "GB_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(GradientBoostingClassifier())),\n",
    "            ])\n",
    "\n",
    "for category in categories:\n",
    "    printmd('**Processing {} Tweets...**'.format(category))\n",
    "    \n",
    "    #Random Over Sampling\n",
    "    if(category != 'HBM Related'):\n",
    "        ros = RandomOverSampler(random_state=777)\n",
    "        ros_xtrain_tfidf, ros_train_y = ros.fit_sample(x_train, train[category])\n",
    "    else:\n",
    "        ros_xtrain_tfidf = x_train\n",
    "        ros_train_y = train[category]\n",
    "    # Training logistic regression model on train data\n",
    "    GB_pipeline.fit(ros_xtrain_tfidf, ros_train_y)\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    predGB = GB_pipeline.predict(x_test)\n",
    "    print(classification_report(test[category], predGB, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Processing HBM Related Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2448,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.45      0.54       434\n",
      "           1       0.69      0.85      0.76       616\n",
      "\n",
      "    accuracy                           0.68      1050\n",
      "   macro avg       0.68      0.65      0.65      1050\n",
      "weighted avg       0.68      0.68      0.67      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Benefits Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3360,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.83       741\n",
      "           1       0.58      0.55      0.57       309\n",
      "\n",
      "    accuracy                           0.75      1050\n",
      "   macro avg       0.70      0.69      0.70      1050\n",
      "weighted avg       0.75      0.75      0.75      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Barriers Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3534,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82       771\n",
      "           1       0.49      0.46      0.48       279\n",
      "\n",
      "    accuracy                           0.73      1050\n",
      "   macro avg       0.65      0.64      0.65      1050\n",
      "weighted avg       0.72      0.73      0.73      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Severity Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4660,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       999\n",
      "           1       0.36      0.31      0.34        51\n",
      "\n",
      "    accuracy                           0.94      1050\n",
      "   macro avg       0.66      0.64      0.65      1050\n",
      "weighted avg       0.94      0.94      0.94      1050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Perceived Susceptibility Tweets...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4674,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      1007\n",
      "           1       0.37      0.40      0.38        43\n",
      "\n",
      "    accuracy                           0.95      1050\n",
      "   macro avg       0.67      0.68      0.68      1050\n",
      "weighted avg       0.95      0.95      0.95      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#logistic regression with oversampling of minority class\n",
    "log_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(max_iter=1000))),\n",
    "            ])\n",
    "\n",
    "for category in categories:\n",
    "    printmd('**Processing {} Tweets...**'.format(category))\n",
    "    \n",
    "    #Random Over Sampling For unbalanced categories\n",
    "    if(category != 'HBM Related'):\n",
    "        ros = RandomOverSampler(random_state=777)\n",
    "        ros_xtrain_tfidf, ros_train_y = ros.fit_sample(x_train, train[category])\n",
    "    else:\n",
    "        ros_xtrain_tfidf = x_train\n",
    "        ros_train_y = train[category]\n",
    "    # Training logistic regression model on train data\n",
    "    log_pipeline.fit(ros_xtrain_tfidf, ros_train_y)\n",
    "    print(ros_train_y.shape)\n",
    "    # calculating test accuracy\n",
    "    predLog = log_pipeline.predict(x_test)\n",
    "    print(classification_report(test[category], predLog, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.04700022803666276, 2099),\n",
       " (0.02165290384386741, 3711),\n",
       " (0.017560750780973078, 'MCPW'),\n",
       " (0.017559941844709766, 'UCC'),\n",
       " (0.017352601318697772, 3944),\n",
       " (0.01705589809579206, 'hasNum'),\n",
       " (0.013732540649165324, 'Pol'),\n",
       " (0.01308159696645004, 'DS'),\n",
       " (0.013031913418364384, 2325),\n",
       " (0.012212217417145019, 'PC')]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a test checking for feature importances\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "ros = RandomOverSampler(random_state=777)\n",
    "ros_xtrain_tfidf, ros_train_y = ros.fit_sample(x_train, train['Perceived Susceptibility'])\n",
    "rf_model = rf.fit(ros_xtrain_tfidf, ros_train_y)\n",
    "sorted(zip(rf_model.feature_importances_, x_train.columns), reverse=True)[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
